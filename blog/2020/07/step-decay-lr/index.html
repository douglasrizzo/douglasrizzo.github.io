<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Reverse engineering a step decay for learning rate | Douglas Meneghetti </title> <meta name="author" content="Douglas De Rizzo Meneghetti"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://douglasrizzo.github.io/blog/2020/07/step-decay-lr/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Douglas Meneghetti </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Reverse engineering a step decay for learning rate</h1> <p class="post-meta"> July 16, 2020 </p> <p class="post-tags"> <a href="/blog/2020"> <i class="fa-solid fa-calendar fa-sm"></i> 2020 </a>   ·   <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> machine-learning</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <ul> <li><a href="#the-theory-behind-the-problem">The theory behind the problem</a></li> <li><a href="#problem-setting">Problem setting</a></li> <li><a href="#finding-the-unknowns">Finding the unknowns</a></li> <li><a href="#a-concrete-example">A concrete example</a></li> <li><a href="#a-pytorch-example">A PyTorch example</a></li> <li><a href="#closing-remarks">Closing remarks</a></li> </ul> <h2 id="the-theory-behind-the-problem">The theory behind the problem</h2> <p>When training a neural network using stochastic gradient descent, the learning rate is a parameter that controls the magnitude of updates to the neural network weights. Using this parameter is important since, when working with minibatch training, updates may not always be in the correct direction and magnitude to minimize the loss function, since the minibatch represents only a fraction of the training data.</p> <p>For that reason, we start with a large learning rate, which allows the network to make large updates to its weights, learning a lot faster. When training is reaching its end, we slowly decrease the learning rate, forcing the network to make smaller updates towards convergence.</p> <h2 id="problem-setting">Problem setting</h2> <p>We would like to implement a decaying strategy for the learning rate $\eta$ of your neural network. One option is to implement step decay for $\eta$, which multiplies $\eta$ by a factor $\gamma$ after every $n$ epochs. This is implemented in libraries such as PyTorch/TensorFlow as <code class="language-plaintext highlighter-rouge">ExponentialLR</code>, <code class="language-plaintext highlighter-rouge">StepLR</code> or <code class="language-plaintext highlighter-rouge">MultiStepLR</code>. These implementations usually expect you to provide the values for the initial learning rate $\eta_0$, $\gamma$ and $n$, but they don’t allow you to explicitly select the final value for $\eta$ at the end of training.</p> <p>We’d like to start at a reasonably high $\eta_0$ and end in an equally reasonable final value $\eta_m$ after $m$ updates. If $\eta_m$ is too low, there is the risk that additional training epochs will not have any effect in the network. If it is too high, our network may not reap the benefits of a low learning rate, which stabilizes convergence. In short:</p> <blockquote> <p>Having full control of the range of values the learning rate assumes would be very beneficial, but the decay strategies provided by DL frameworks don’t make this task so easy to achieve, because they allow us to use a less obvious set of variables that indirectly affect the values assumed by the learning rate during training.</p> </blockquote> <p>This setting may be familiar in deep reinforcement learning, in which a neural network may need to be trained for many epochs, to let the agent generate new transitions with updated policies, while also learning faster at the beginning of training.</p> <p>In this post, given the number of updates we will apply to the learning rate (which we have called $n$), we’ll see:</p> <ul> <li>how we can select the range of values the learning rate may assume ($\eta_0 \geq \eta \geq \eta_m$);</li> <li>select an appropriate value for the step decay parameter $\gamma$ that keeps $\eta$ in that range.</li> </ul> <h2 id="finding-the-unknowns">Finding the unknowns</h2> <p>At update step $m$, the value of $\eta_m=\eta_0 \cdot \gamma^m$. If we take $\eta_m$ as the final possible value of $\eta$ and, by consequence, its lowest one, and select a reasonable value for it, we can then solve for $\gamma$:</p> <p>\[ \eta_m=\eta_0 \cdot \gamma^m \]</p> <p>\[ \frac{\eta_m}{\eta_0}=\gamma^m \]</p> <p>\[ \gamma=\sqrt[m]{\frac{\eta_m}{\eta_0}} \]</p> <p>The value of $m$ can be taken as the number of times $\eta_0$ will be multiplied by $\gamma$ until the end of training. Bear in mind that, while optimally, we would like to update $\eta$ after every epoch (making $m=M$), this can introduce numerical errors, since $\gamma$ would need to be too close to 1 for updates to be small enough. Instead, we set $m &lt; M$ and tell PyTorch/TensorFlow to update $\eta$ after a number of steps $n$ such that, by the end of training, $\eta$ has been updated $m$ times.</p> <p>This value $n$ can be found by dividing the total number of epochs $M$ by the number of updates $m$.</p> <h2 id="a-concrete-example">A concrete example</h2> <ul> <li>Number of training epochs $M=1000000=10^6$</li> <li>Initial learning rate $\eta_0=0.1$</li> <li>Final learning rate $\eta_m=0.0001$</li> <li>Number of updates to $\eta$ until the end of training $m=10000=10^4$</li> <li>Step decay $\gamma=\sqrt[m]{\frac{\eta_M}{\eta_0}}=\sqrt[10000]{\frac{0.0001}{0.1}} \approx 0.999309463003$</li> <li>Number of training steps between updates $n=\frac{M}{m}=\frac{10^6}{10^4}=100$</li> </ul> <h2 id="a-pytorch-example">A PyTorch example</h2> <p>In this example, I create an SGD optimizer for my network and show how we can accomplish our bounded learning rate using a <code class="language-plaintext highlighter-rouge">StepLR</code>, <code class="language-plaintext highlighter-rouge">MultiStepLR</code> or <code class="language-plaintext highlighter-rouge">ExponentialLR</code> scheduler available in PyTorch.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">max_steps</span>  <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="mf">1E6</span><span class="p">)</span>                                 <span class="c1"># M
</span><span class="n">n_updates</span>  <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="mf">1E4</span><span class="p">)</span>                                 <span class="c1"># n
</span><span class="n">initial_lr</span> <span class="o">=</span> <span class="mf">0.1</span>                                      <span class="c1"># alpha_0
</span><span class="n">final_lr</span>   <span class="o">=</span> <span class="p">.</span><span class="mi">0001</span>                                    <span class="c1"># alpha_M
</span><span class="n">step_size</span>  <span class="o">=</span> <span class="n">max_steps</span> <span class="o">//</span> <span class="n">n_updates</span>                   <span class="c1"># n
</span><span class="n">lr_gamma</span>   <span class="o">=</span> <span class="p">(</span><span class="n">final_lr</span> <span class="o">/</span> <span class="n">initial_lr</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">n_updates</span><span class="p">)</span> <span class="c1"># gamma
</span>
<span class="n">optimizer</span>  <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span>
                       <span class="n">lr</span><span class="o">=</span><span class="n">initial_lr</span><span class="p">)</span>

<span class="c1"># using our step size n
</span><span class="n">scheduler</span>  <span class="o">=</span> <span class="nc">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="n">step_size</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">lr_gamma</span><span class="p">)</span>

<span class="c1"># or using a list of milestone steps
</span><span class="n">scheduler</span>  <span class="o">=</span> <span class="nc">MultiStepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span>
                         <span class="n">milestones</span><span class="o">=</span><span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">step_size</span><span class="p">,</span> <span class="n">max_steps</span><span class="p">,</span> <span class="n">step_size</span><span class="p">)),</span>
                         <span class="n">gamma</span><span class="o">=</span><span class="n">lr_gamma</span><span class="p">)</span>

<span class="c1"># or using the "close to 1" unstable gamma
</span><span class="n">unstable_gamma</span> <span class="o">=</span> <span class="p">(</span><span class="n">final_lr</span> <span class="o">/</span> <span class="n">initial_lr</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">max_steps</span><span class="p">)</span>
<span class="n">scheduler</span>  <span class="o">=</span> <span class="nc">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">unstable_gamma</span><span class="p">)</span>
</code></pre></div></div> <h2 id="closing-remarks">Closing remarks</h2> <p>In this post, I have shown you how I have solved the problem of bounding the learning rate of a neural network between a maximum and minimum value, by analitically finding the correct multiplicative decay factor for a given number of updates that will be applied to the learning rate until the end of training.</p> <p>I hope this post helps other people. Bye.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/03/local-gemma-chatbot-langchain-ollama/">Running a Gemma-powered question-answering chatbot locally with LangChain + Ollama</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/02/llm-qa-obsidian-rag/">Answering questions from an Obsidian database with LLMs + RAG</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/07/ts-queue-experiments/">Using task-spooler to queue experiments on Linux</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/06/mathematics-self-study/">Resources to self-study mathematics for machine learning</a> </li> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Douglas De Rizzo Meneghetti. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>