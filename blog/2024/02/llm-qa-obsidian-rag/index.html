<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Answering questions from an Obsidian database with LLMs + RAG | Douglas Meneghetti </title> <meta name="author" content="Douglas De Rizzo Meneghetti"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://douglasrizzo.github.io/blog/2024/02/llm-qa-obsidian-rag/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Douglas Meneghetti </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Answering questions from an Obsidian database with LLMs + RAG</h1> <p class="post-meta"> February 19, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/llms"> <i class="fa-solid fa-hashtag fa-sm"></i> llms</a>   <a href="/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> nlp</a>   <a href="/blog/tag/python"> <i class="fa-solid fa-hashtag fa-sm"></i> python</a>   <a href="/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> neural-networks</a>     ·   <a href="/blog/category/nlp"> <i class="fa-solid fa-tag fa-sm"></i> nlp</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>This notebook presents a proof-of-concept on how to create a question-answering bot powered by an LLM and with knowledge extracted from actual documents, more specifically, a collection of notes from Obsidian.</p> <p>I am going to run all experiments locally, using a 7th gen i7, an NVIDIA GTX 1070 and 32 GBs of RAM.</p> <p>A lot of the heavy lifting will be done by the <a href="https://www.langchain.com/" rel="external nofollow noopener" target="_blank">LangChain</a> package, which I am on the process of learning to use.</p> <p>On the road to building this Q&amp;A bot, we will be introduced to many concepts:</p> <ol> <li>Document loading</li> <li>Text data cleaning using regex</li> <li>Splitting of Markdown documents into text chunks</li> <li>Sentence embeddings</li> <li>Vector stores</li> <li>Similarity search using cosine similarity between embedding vectors</li> <li>maximal marginal relevance search</li> <li>Self-query retrieval</li> <li>Contextual compression retrieval</li> <li>Question-answering with retrieval augmented generation</li> <li>Question-answering with retrieval augmented generation and custom templates</li> </ol> <p>So that no one gets lost, the following diagram explains how the whole pipeline to get to our final Q&amp;A bot (and how this notebook) works:</p> <p><img src="/assets/img/langsidian.png" alt="My happy picture"></p> <h2 id="prerequisites">Prerequisites</h2> <ol> <li>You can find the environment.yml file to create a conda env with the necessary dependencies to run the notebook.</li> <li> <p>You should download a model from the GPT4All website and save it on <code class="language-plaintext highlighter-rouge">./models/my_little_llm.gguf</code>. The one below is the one I used.</p> <div class="language-sh highlighter-rouge"> <div class="highlight"><pre class="highlight"><code> wget https://gpt4all.io/models/gguf/mistral-7b-openorca.Q4_0.gguf <span class="nt">-O</span> models/my_little_llm.gguf
</code></pre></div> </div> </li> </ol> <h2 id="loading-documents">Loading documents</h2> <p>Here I used the <code class="language-plaintext highlighter-rouge">ObsidianLoader</code> document loader and point it to the directory that contains all my notes in Markdown format.</p> <p>We can see I have ~500 text files.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="n">docs_path</span> <span class="o">=</span> <span class="p">(</span><span class="n">Path</span><span class="p">.</span><span class="nf">home</span><span class="p">()</span> <span class="o">/</span> <span class="sh">"</span><span class="s">Documents</span><span class="sh">"</span> <span class="o">/</span> <span class="sh">"</span><span class="s">Obsidian</span><span class="sh">"</span><span class="p">).</span><span class="nf">absolute</span><span class="p">()</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain_community.document_loaders</span> <span class="kn">import</span> <span class="n">ObsidianLoader</span>

<span class="n">loader</span> <span class="o">=</span> <span class="nc">ObsidianLoader</span><span class="p">(</span><span class="n">docs_path</span><span class="p">,</span> <span class="n">collect_metadata</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="sh">"</span><span class="s">UTF-8</span><span class="sh">"</span><span class="p">)</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">loader</span><span class="p">.</span><span class="nf">load</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Loaded </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span><span class="si">}</span><span class="s"> docs</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Encountered non-yaml frontmatter


Loaded 499 docs
</code></pre></div></div> <p>Let’s take a peek at one of the documents. We can see it has the textual content itself, as well as some metadata. The <code class="language-plaintext highlighter-rouge">ObsidianLoader</code> includes file properties from Obsidian documents, such as tags, dates and aliases, as part of the metadata.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">docs</span><span class="p">[</span><span class="mi">7</span><span class="p">]</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Document(page_content='A method for pre-training [[language model]]s in which the model has access to the first tokens of the sequence and its task is to predict the next token.\n\nThe following examples depict how a single sequence can be turned into multiple training examples:\n\n1. `&lt;START&gt;` → `the`\n1. `&lt;START&gt; the` → `teacher`\n1. `&lt;START&gt; the teacher` → `teaches`\n1. `&lt;START&gt; the teacher teaches` → `the`\n1. `&lt;START&gt; the teacher teaches the` → `student`\n1. `&lt;START&gt; the teacher teaches the student` → `&lt;END&gt;`\n\nModels trained using this method have access to the full sequence of tokens at inference time, making them appropriate for non-generative tasks that revolve around processing a sequence of tokens as a whole, for example:\n\n- [[Sentiment Analysis]]\n- [[Named entity recognition]]\n- [[Word classification]]\n\n[[Bidirectional Encoder Representation from Transformers|BERT]] is an example of a masked language model. Example from [[Bidirectional Encoder Representation from Transformers|BERT]]: Choose 15% of the tokens at random: mask them 80% of the time, replace them with a random token 10% of the time, or keep as is 10% of the time.\n\n## Sources\n\n- [[DeepLearning.AI Natural Language Processing Specialization]]\n- [[Generative AI with Large Language Models]]', metadata={'source': 'Causal language modeling.md', 'path': '/home/dodo/Documents/Obsidian/Causal language modeling.md', 'created': 1700448369.2719378, 'last_modified': 1700448369.2719378, 'last_accessed': 1708267659.2105181, 'tags': 'area/ai/nlp/llm', 'date': '2023-11-19 23:41'})
</code></pre></div></div> <h2 id="cleaning-documents">Cleaning documents</h2> <p>Obsidian documents have some of their own Markdown flavor, like <code class="language-plaintext highlighter-rouge">[[Graph Neural Network|GNNs]]</code>, where <code class="language-plaintext highlighter-rouge">Graph Neural Network</code> is the name of a document and <code class="language-plaintext highlighter-rouge">GNNs</code> is what appears on the text. In cases like these, we want to keep only the second part.</p> <p>It also has full-on links, such as <code class="language-plaintext highlighter-rouge">[[grid world]]</code>, in which case we want to remove the double brackets.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># !pip install google-re2
</span><span class="kn">import</span> <span class="n">re2</span>

<span class="n">docus</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">insane_pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="sh">"</span><span class="s">\[\[([^\]]*?)\|([^\[]*?)\]\]</span><span class="sh">"</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">:</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">re2</span><span class="p">.</span><span class="nf">search</span><span class="p">(</span><span class="n">insane_pattern</span><span class="p">,</span> <span class="n">doc</span><span class="p">.</span><span class="n">page_content</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">new_doc</span> <span class="o">=</span> <span class="n">re2</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="n">insane_pattern</span><span class="p">,</span> <span class="sa">r</span><span class="sh">"</span><span class="s">\2</span><span class="sh">"</span><span class="p">,</span> <span class="n">doc</span><span class="p">.</span><span class="n">page_content</span><span class="p">)</span>
        <span class="n">docus</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="n">doc</span><span class="p">.</span><span class="n">page_content</span><span class="p">,</span>
                <span class="n">new_doc</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="n">doc</span><span class="p">.</span><span class="n">page_content</span> <span class="o">=</span> <span class="n">new_doc</span>
    <span class="n">doc</span><span class="p">.</span><span class="n">page_content</span> <span class="o">=</span> <span class="n">doc</span><span class="p">.</span><span class="n">page_content</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">"</span><span class="s">[[</span><span class="sh">"</span><span class="p">,</span> <span class="sh">""</span><span class="p">).</span><span class="nf">replace</span><span class="p">(</span><span class="sh">"</span><span class="s">]]</span><span class="sh">"</span><span class="p">,</span> <span class="sh">""</span><span class="p">)</span>

<span class="nf">sorted</span><span class="p">(</span><span class="n">docus</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]))[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>('- [[Intersection over Union|IoU]]', '- IoU')
</code></pre></div></div> <h2 id="splitting-documents">Splitting documents</h2> <p>This step splits the documents loaded in the previous step into smaller chunks.</p> <p>LangChain provides its own Markdown text splitter, which we are going to use.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">MarkdownTextSplitter</span>

<span class="n">splitter</span> <span class="o">=</span> <span class="nc">MarkdownTextSplitter</span><span class="p">(</span><span class="n">chunk_size</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">splits</span> <span class="o">=</span> <span class="n">splitter</span><span class="p">.</span><span class="nf">split_documents</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>


<span class="nf">len</span><span class="p">(</span><span class="n">splits</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1510
</code></pre></div></div> <p>Let’s take a peek at a chunk. They inherit the metadata of their parent document.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">splits</span><span class="p">[</span><span class="mi">542</span><span class="p">]</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Document(page_content='# epsilon-soft policies\n\nAn $\\epsilon$-soft policy is a stochastic policy that always assigns a non-zero $\\frac{\\epsilon}{|A|}$ probability to all actions. These policies always perform some exploration.\n\nThe uniform random policy is an $\\epsilon$-soft policy. The epsilon-greedy policy also is.', metadata={'source': 'epsilon-soft policies.md', 'path': '/home/dodo/Documents/Obsidian/epsilon-soft policies.md', 'created': 1680669506.8282943, 'last_modified': 1680669506.8282943, 'last_accessed': 1708267660.780534, 'tags': 'area/ai/rl project/rl-spec', 'aliases': 'epsilon-soft policy', 'date': '2021-05-24 18:32'})
</code></pre></div></div> <h2 id="computing-embeddings-and-saving-them-to-a-vector-store">Computing embeddings and saving them to a vector store</h2> <p>To quickly search for text chunks, it is useful to precompute an embedding vector for each chunk and store it for future use.</p> <p>An embedding vector is a numerical vector that represents the text chunk. It allows us to compare chunks in the embedding space. Chunks with similar semantic meaning tend to have similar embedding vectors. This similarity can be computed using e.g. cosine similarity.</p> <p>My choice for embedding generator was <a href="https://www.sbert.net/" rel="external nofollow noopener" target="_blank">SentenceTransformers</a>, provided by Hugging Face, which runs locally.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># !pip install sentence_transformers
</span><span class="kn">from</span> <span class="n">langchain.embeddings.huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEmbeddings</span>

<span class="n">embedding</span> <span class="o">=</span> <span class="nc">HuggingFaceEmbeddings</span><span class="p">()</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/home/dodo/.anaconda3/envs/langsidian/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
</code></pre></div></div> <p>Computed embedding vectors can be stored in <em>vector stores</em>. The one we will use in this project is <a href="https://docs.trychroma.com/" rel="external nofollow noopener" target="_blank">Chroma</a>. It is free, runs locally and is perfect for our small document base.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># !pip install chromadb
</span><span class="kn">from</span> <span class="n">langchain.vectorstores</span> <span class="kn">import</span> <span class="n">Chroma</span>

<span class="n">persist_directory</span> <span class="o">=</span> <span class="sh">"</span><span class="s">docs/chroma/</span><span class="sh">"</span>
<span class="err">!</span><span class="n">rm</span> <span class="o">-</span><span class="n">rf</span> <span class="p">.</span><span class="o">/</span><span class="n">docs</span><span class="o">/</span><span class="n">chroma</span>  <span class="c1"># remove old database files if any
</span><span class="n">vectordb</span> <span class="o">=</span> <span class="n">Chroma</span><span class="p">.</span><span class="nf">from_documents</span><span class="p">(</span>
    <span class="n">documents</span><span class="o">=</span><span class="n">splits</span><span class="p">,</span> <span class="n">embedding</span><span class="o">=</span><span class="n">embedding</span><span class="p">,</span> <span class="n">persist_directory</span><span class="o">=</span><span class="n">persist_directory</span>
<span class="p">)</span>
<span class="n">vectordb</span><span class="p">.</span><span class="n">_collection</span><span class="p">.</span><span class="nf">count</span><span class="p">()</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1510
</code></pre></div></div> <h2 id="retrieval">Retrieval</h2> <p>Retrieval is the act of retrieving text chunks from our vector store, given an input prompt.</p> <p>Basic retrieval is performed by comparing the prompt embedding with those of the text chunks. More complex retrieval techniques involve calls to an LLM.</p> <h3 id="basic-retrieval">Basic retrieval</h3> <p>Let’s first test a retrieval technique based on similarity search in the vector store. Given a prompt, the procedure should return the most similar or relevant chunks in the vector database.</p> <p>The question below will be used as a test for everything else below in the notebook. It is related to reinforcement learning, an area in which I have a few hundred documents written on Obsidian. You can find more about the question <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#value-functions" rel="external nofollow noopener" target="_blank">here</a> to see if our retrieval methods actually nail the answer.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">question</span> <span class="o">=</span> <span class="sh">"</span><span class="s">What is the definition of the action value function?</span><span class="sh">"</span>
</code></pre></div></div> <p>The first example of retrieval is <em>similarity search</em>, which will convert the prompt into an embedding vector and compute the cosine similarity between the prompt embedding and the embeddings of all chunks in the vector store, returning the <em>k</em> most similar chunks.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">retrieved_docs</span> <span class="o">=</span> <span class="n">vectordb</span><span class="p">.</span><span class="nf">similarity_search</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">retrieved_docs</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">doc</span><span class="p">.</span><span class="n">page_content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="sh">"</span><span class="se">\n\n</span><span class="s">---</span><span class="se">\n\n</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The action-value function represents the expected return from a given state after taking a specific action and later following a specific policy.

$$q_{\pi}(s,a)=\mathbb{E}_{\pi}[G_t|S_t=s,A_t=a]$$

where $G_t$ is the Expected sum of future rewards.

---

A value function maps states, or state-action pairs, to expected returns.

- State-value function
- Action-value function

---

The state-value function represents the expected return from a given state, possibly under a given policy.

$$v(s)=\mathbb{E}[G_t|S_t=s]$$
$$v_{\pi}(s)=\mathbb{E}_{\pi}[G_t|S_t=s]$$

where $G_t$ is the Expected sum of future rewards.

---

The same goes for the Action-value function.

$$\begin{align}
q_*(s,a) &amp; = \sum_{s'}\sum_r p(s',r|s,a)[r + \gamma \sum_{a'} \pi_*(a'|s') q_*(s',a')] \\
       &amp; = \sum_{s'}\sum_r p(s',r|s,a)[r + \gamma \max_{a'} q_*(s',a')]
\end{align}$$

---

Let's say we have a policy $\pi_1$ that has a value function $v_{\pi_1}$. If we use $v_{\pi_1}$ to evaluate states but, instead of following $\pi_1$, we actually always select the actions that will take us to the future state $s'$ with highest $v_{\pi_1}(s')$, we will end up with a policy $\pi_2$ that is equal to or better than $\pi_1$.

---

$$\begin{align}
v_*(s) &amp; = \sum_a \pi_*(a|s) &amp; \sum_{s'}\sum_r p(s',r|s,a)[r + \gamma v_*(s')] \\
       &amp; = \max_a &amp; \sum_{s'}\sum_r p(s',r|s,a)[r + \gamma v_*(s')]
\end{align}$$
where $\pi_*$ is the Optimal policy.

The same goes for the Action-value function.

---

It's a function that dictates the probability the state will find itself in an arbitrary state $s'$ and the agent will receive reward $r$, given the current state the environment finds itself in, $s$, and the action chosen by the agent in $s$, depicted as $a$. It is usually denoted as $p(s',r|s,a)$.

Some properties of this function:

---

Policy evaluation is the task of finding the state-value function $v_{\pi}$, given the policy $\pi$. ^1b9b46

---
</code></pre></div></div> <h4 id="maximal-marginal-relevance-search">Maximal marginal relevance search</h4> <p>Plain similarity search has a drawback. It tends to recover chunks which are very similar or even identical, diminishing the overall amount of information present in the retrieved chunks.</p> <p>To solve this, LangChain provides a method called maximal marginal relevance search, which works by <em>“[…] finding the examples with the embeddings that have the greatest cosine similarity with the inputs, and then iteratively adding them while penalizing them for closeness to already selected examples.”</em> <a href="https://python.langchain.com/docs/modules/model_io/prompts/example_selector_types/mmr" rel="external nofollow noopener" target="_blank">[source]</a></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">retrieved_docs</span> <span class="o">=</span> <span class="n">vectordb</span><span class="p">.</span><span class="nf">max_marginal_relevance_search</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">retrieved_docs</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">doc</span><span class="p">.</span><span class="n">page_content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="sh">"</span><span class="se">\n</span><span class="s">---</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The action-value function represents the expected return from a given state after taking a specific action and later following a specific policy.

$$q_{\pi}(s,a)=\mathbb{E}_{\pi}[G_t|S_t=s,A_t=a]$$

where $G_t$ is the Expected sum of future rewards.
---
A value function maps states, or state-action pairs, to expected returns.

- State-value function
- Action-value function
---
A generalization of Sarsa which employs the n-step return for the action value function,

!n-step return#^205a30 ^68659e

This estimate is then used in the following update rule for the action-value of the state-action pair at time $t$.

$$Q_{t+n}(S_t, A_t) \doteq Q_{t+n-1}(S_t, A_t) + \alpha [G_{t:t+n} - \gamma^n Q_{t+n-1}(S_t, A_t)]$$ ^ca04db
---
- if the agent exploits without having a good estimate of the action-value function, it will most likely be locked in suboptimal behavior, not being able to gather information from unknown transitions which might bring it more return.
---
Some properties of this function:

It maps states and actions to states and rewards, so its cardinality is $$p:S \times R \times S \times A \to [0;1]$$

It is a probability, so the sum over all possible combinations of states and rewards must be one,
$$\sum_{s' \in S} \sum_{r \in R} p(s',r|s,a) = 1, \forall s \in S, a \in A(s)$$
---
# Factored value functions in cooperative multi-agent reinforcement learning

&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/W_9kcQmaWjo?start=684" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;

VDN was the first one and the one I used in my Doctorate.
---
- Exploitation: select the greedy action with relation to the action-value function.
- Exploration: select a non-greedy action.
---
Given the following MDP:

!Pasted image 20210523192818.png

The Bellman equation allows the value function to be expressed and solved as a system of linear equations: ^c06dd9

!Bellman equation for the state-value function#^a65ad4
---
</code></pre></div></div> <h3 id="llm-backed-retrieval">LLM-backed retrieval</h3> <p>Some retrieval techniques require an underlying language model to be performed. The LLM may be used to, e.g. summarize or make chunks more coherent before returning them.</p> <h4 id="instantiating-the-llm">Instantiating the LLM</h4> <p>The LLM I chose is <a href="https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca" rel="external nofollow noopener" target="_blank">Mistral-7B-OpenOrca</a>, provided by <a href="https://gpt4all.io/index.html" rel="external nofollow noopener" target="_blank">GPT4All</a>.</p> <ul> <li> <a href="https://mistral.ai/news/announcing-mistral-7b/" rel="external nofollow noopener" target="_blank">Mistral 7B</a> is the best free and open 7 billion parameter LLM. It is also small enough to run on my GPU.</li> <li>The <a href="https://huggingface.co/datasets/Open-Orca/OpenOrca" rel="external nofollow noopener" target="_blank">OpenOrca dataset</a> is a conversation dataset.</li> <li>According to <a href="https://mistral.ai/product/" rel="external nofollow noopener" target="_blank">Mistral’s product website</a>, this model has an 8k context window, which we should consider when retrieving chunks for it to process.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># !pip install gpt4all
# !pip install lark
# !wget https://gpt4all.io/models/gguf/mistral-7b-openorca.Q4_0.gguf -O models/my_little_llm.gguf
# !wget https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf -O models/my_little_llm.gguf
</span><span class="kn">from</span> <span class="n">langchain_community.llms.gpt4all</span> <span class="kn">import</span> <span class="n">GPT4All</span>

<span class="n">llm</span> <span class="o">=</span> <span class="nc">GPT4All</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">models/my_little_llm.gguf</span><span class="sh">"</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">gpu</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>llama.cpp: using Vulkan on NVIDIA GeForce GTX 1070
</code></pre></div></div> <h4 id="self-query-retrieval">Self-query retrieval</h4> <p>Self-query is a technique in which an LLM is specifically prompted to output a <em>structured query</em>. It also allows it to take document/chunk metadata into consideration, as long as we describe each attribute in the metadata with a textual description.</p> <p>Under the hood, self-query performs some pretty convoluted modifications to the original prompt and I advise you look at the documentation to understand what’s going on. <a href="https://python.langchain.com/docs/modules/data_connection/retrievers/self_query#constructing-from-scratch-with-lcel" rel="external nofollow noopener" target="_blank">[Source]</a></p> <p>As we have seen when inspecting our splits, we can see that our data includes metadata taken from the file properties of Obsidian documents. We will go ahead and described them as attributes for the self-query retriever.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.chains.query_constructor.base</span> <span class="kn">import</span> <span class="n">AttributeInfo</span>

<span class="n">metadata_field_info</span> <span class="o">=</span> <span class="p">[</span>
    <span class="nc">AttributeInfo</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">source</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">The name of the Markdown file that contained the chunk. If you ignore the .md extension, it is the name of the article the chunk came from.</span><span class="sh">"</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="sh">"</span><span class="s">string</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="nc">AttributeInfo</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">aliases</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">Other names for the article the chunk came from, if any.</span><span class="sh">"</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="sh">"</span><span class="s">string</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="nc">AttributeInfo</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">tags</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">A series of comma-separated tags that categorize the article the chunk came from. When a tags starts with </span><span class="sh">'</span><span class="s">area</span><span class="sh">'</span><span class="s">, it denotes a broad area of knowledge. When it starts with </span><span class="sh">'</span><span class="s">project</span><span class="sh">'</span><span class="s">, it describes a specific project with beginning and end.</span><span class="sh">"</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="sh">"</span><span class="s">string</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="nc">AttributeInfo</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">authors</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">When the document summarizes a scientific paper, this attribute holds a comma-separated list of author names.</span><span class="sh">"</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="sh">"</span><span class="s">string</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="nc">AttributeInfo</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">year</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">When the document summarizes a scientific paper, this attribute contains the year of the publication.</span><span class="sh">"</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="sh">"</span><span class="s">integer</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">),</span>
<span class="p">]</span>
<span class="n">document_content_description</span> <span class="o">=</span> <span class="sh">"</span><span class="s">A collection of study notes in Markdown format written by a single author, mostly about artificial intelligence topics.</span><span class="sh">"</span>
</code></pre></div></div> <p>The self-query retriever can also be configured to use maximal marginal relevance search, as you can see in the <code class="language-plaintext highlighter-rouge">base_retriever</code> argument below.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.retrievers.self_query.base</span> <span class="kn">import</span> <span class="n">SelfQueryRetriever</span>

<span class="n">retriever</span> <span class="o">=</span> <span class="n">SelfQueryRetriever</span><span class="p">.</span><span class="nf">from_llm</span><span class="p">(</span>
    <span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span>
    <span class="n">vectorstore</span><span class="o">=</span><span class="n">vectordb</span><span class="p">,</span>
    <span class="n">document_contents</span><span class="o">=</span><span class="n">document_content_description</span><span class="p">,</span>
    <span class="n">metadata_field_info</span><span class="o">=</span><span class="n">metadata_field_info</span><span class="p">,</span>
    <span class="n">base_retriever</span><span class="o">=</span><span class="n">vectordb</span><span class="p">.</span><span class="nf">as_retriever</span><span class="p">(</span><span class="n">search_type</span><span class="o">=</span><span class="sh">"</span><span class="s">mmr</span><span class="sh">"</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">8</span><span class="p">),</span>
<span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">retriever</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Document(page_content='!Pasted image 20231129031306.png', metadata={'created': 1708307272.9665868, 'date': '2023-11-29 01:34', 'last_accessed': 1708307272.9699202, 'last_modified': 1708307272.9665868, 'path': '/home/dodo/Documents/Obsidian/Single linkage.md', 'source': 'Single linkage.md', 'tags': 'area/ai/ml/clustering'}),
 Document(page_content='!Pasted image 20230317051147.png', metadata={'created': 1680667926.0942817, 'date': '2023-03-17 04:33', 'last_accessed': 1708267663.323892, 'last_modified': 1680667926.0942817, 'path': '/home/dodo/Documents/Obsidian/Comparing feature vectors in NLP.md', 'source': 'Comparing feature vectors in NLP.md', 'tags': 'area/ai/nlp project/nlp-spec'}),
 Document(page_content='!Pasted image 20230325081439.png', metadata={'created': 1679742881.9000912, 'last_accessed': 1708267661.4972079, 'last_modified': 1679742881.9000912, 'path': '/home/dodo/Documents/Obsidian/Text cleaning.md', 'source': 'Text cleaning.md'}),
 Document(page_content='!_attachments/Pasted image 20210523185724.png', metadata={'created': 1680669713.8148472, 'date': '2023-04-05 01:41', 'last_accessed': 1708267662.710553, 'last_modified': 1680669713.8148472, 'path': '/home/dodo/Documents/Obsidian/Iterative policy evaluation.md', 'source': 'Iterative policy evaluation.md', 'tags': 'area/ai/rl project/rl-spec'})]
</code></pre></div></div> <p>As we can see, without more informative metadata (or better preprocessing of the text documents), the retrieved chunks are not very useful. It only retrieved chunks related to figures.</p> <h3 id="contextual-compression-retrieval">Contextual compression retrieval</h3> <p>As a final test on retrieval, we will implement a <em>“contextual compression retriever”</em>.</p> <p>From the <a href="https://python.langchain.com/docs/modules/data_connection/retrievers/contextual_compression" rel="external nofollow noopener" target="_blank">LangChain documentation</a>:</p> <blockquote> <p>The Contextual Compression Retriever passes queries to the base retriever, takes the initial documents and passes them through the Document Compressor. The Document Compressor takes a list of documents and shortens it by reducing the contents of documents or dropping documents altogether.</p> </blockquote> <p>In our case:</p> <ul> <li>The base retriever will be a maximal marginal similarity search.</li> <li>The compressor will be Mistral-7b-OpenOrca.</li> </ul> <p>Our hope is that the small, irrelevant chunks returned by the self-query retriever will be dropped and more relevant chunks will be summarized and returned.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.retrievers</span> <span class="kn">import</span> <span class="n">ContextualCompressionRetriever</span>
<span class="kn">from</span> <span class="n">langchain.retrievers.document_compressors</span> <span class="kn">import</span> <span class="n">LLMChainExtractor</span>

<span class="n">compressor</span> <span class="o">=</span> <span class="n">LLMChainExtractor</span><span class="p">.</span><span class="nf">from_llm</span><span class="p">(</span><span class="n">llm</span><span class="p">)</span>
<span class="n">compression_retriever</span> <span class="o">=</span> <span class="nc">ContextualCompressionRetriever</span><span class="p">(</span>
    <span class="n">base_compressor</span><span class="o">=</span><span class="n">compressor</span><span class="p">,</span> <span class="n">base_retriever</span><span class="o">=</span><span class="n">vectordb</span><span class="p">.</span><span class="nf">as_retriever</span><span class="p">(</span><span class="n">search_type</span><span class="o">=</span><span class="sh">"</span><span class="s">mmr</span><span class="sh">"</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">compressed_docs</span> <span class="o">=</span> <span class="n">compression_retriever</span><span class="p">.</span><span class="nf">get_relevant_documents</span><span class="p">(</span><span class="n">question</span><span class="p">)</span>
<span class="n">compressed_docs</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/home/dodo/.anaconda3/envs/langsidian/lib/python3.12/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.
  warnings.warn(
/home/dodo/.anaconda3/envs/langsidian/lib/python3.12/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.
  warnings.warn(
/home/dodo/.anaconda3/envs/langsidian/lib/python3.12/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.
  warnings.warn(
/home/dodo/.anaconda3/envs/langsidian/lib/python3.12/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.
  warnings.warn(





[Document(page_content='The action-value function represents the expected return from a given state after taking a specific action and later following a specific policy.', metadata={'created': 1680669936.852964, 'date': '2023-04-05 01:45', 'last_accessed': 1708267660.0105264, 'last_modified': 1680669936.852964, 'path': '/home/dodo/Documents/Obsidian/Action-value function.md', 'source': 'Action-value function.md', 'tags': 'area/ai/rl project/rl-spec'}),
 Document(page_content='Action-value function', metadata={'created': 1680669672.8131003, 'date': '2023-04-05 01:41', 'last_accessed': 1708267659.7238567, 'last_modified': 1680669672.8131003, 'path': '/home/dodo/Documents/Obsidian/Value functions.md', 'source': 'Value functions.md', 'tags': 'area/ai/rl project/rl-spec'}),
 Document(page_content='*NO_OUTPUT*\n\nThe definition of the action value function is not mentioned in this context.', metadata={'created': 1633628586.5949209, 'date': '2021-03-02 23:01', 'last_accessed': 1708267661.2005382, 'last_modified': 1632030179.7187316, 'path': '/home/dodo/Documents/Obsidian/Factored value functions in cooperative multi-agent reinforcement learning.md', 'source': 'Factored value functions in cooperative multi-agent reinforcement learning.md', 'tags': 'None'}),
 Document(page_content='Action-Value Function Definition: Not mentioned in the context.', metadata={'created': 1680669515.4254303, 'date': '2023-04-05 01:38', 'last_accessed': 1708267661.453874, 'last_modified': 1680669515.4254303, 'path': '/home/dodo/Documents/Obsidian/Exploration-exploitation tradeoff.md', 'source': 'Exploration-exploitation tradeoff.md', 'tags': 'area/ai/rl project/rl-spec'})]
</code></pre></div></div> <p>These results seem much better than the previous ones, but they are still just a collection of chunks. When interacting with LLMs and chatbots in general, we expect a more direct response.</p> <h2 id="question-answering-using-llms-and-rag">Question-answering using LLMs and RAG</h2> <p>In this example, we will perform retrieval augmented generation for question-answering in an Obsidian document database.</p> <p>To summarize what we already have for this step:</p> <ol> <li>Our documents have been loaded and preprocessed.</li> <li>Chunks have been split from the documents, embedded and stored in the vector store.</li> <li>An LLM has been successfully loaded into memory.</li> </ol> <h3 id="plain-retrieval-qa">Plain retrieval Q&amp;A</h3> <p>This method of Q&amp;A uses the prompt to find relevant chunks in the vector store. These chunks are called the <em>context</em> of the prompt and they are concatenated to the prompt, which is then passed directly to the LLM.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.chains</span> <span class="kn">import</span> <span class="n">RetrievalQA</span>

<span class="n">qa_chain</span> <span class="o">=</span> <span class="n">RetrievalQA</span><span class="p">.</span><span class="nf">from_chain_type</span><span class="p">(</span>
    <span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span> <span class="n">retriever</span><span class="o">=</span><span class="n">vectordb</span><span class="p">.</span><span class="nf">as_retriever</span><span class="p">(</span><span class="n">search_type</span><span class="o">=</span><span class="sh">"</span><span class="s">mmr</span><span class="sh">"</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div> <p>We can see which arguments the chain expects by inspecting the input_keys <code class="language-plaintext highlighter-rouge">list</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">qa_chain</span><span class="p">.</span><span class="n">input_keys</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['query']
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">result</span> <span class="o">=</span> <span class="nf">qa_chain</span><span class="p">({</span><span class="sh">"</span><span class="s">query</span><span class="sh">"</span><span class="p">:</span> <span class="n">question</span><span class="p">})</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/home/dodo/.anaconda3/envs/langsidian/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.
  warn_deprecated(
</code></pre></div></div> <p>The result of prompting the overall system can be seen below. If you remember the definition of the action-value function <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#value-functions" rel="external nofollow noopener" target="_blank">[source]</a>, our Q&amp;A bot has pretty much nailed it!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">result</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'query': 'What is the definition of the action value function?',
 'result': ' The action-value function represents the expected return from a given state after taking a specific action and later following a specific policy.'}
</code></pre></div></div> <p>Under the hood, the <code class="language-plaintext highlighter-rouge">RetrievalQA</code> object uses a prompt template into which it replaces the context and the question before sending the full text prompt to the LLM. We can see it by inspecting the object’s graph.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">qa_chain</span><span class="p">.</span><span class="nf">get_graph</span><span class="p">().</span><span class="n">nodes</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'7eac904b44594e20852d8f0519ef0c3e': Node(id='7eac904b44594e20852d8f0519ef0c3e', data=&lt;class 'pydantic.v1.main.ChainInput'&gt;),
 '8e824ec8c0654d0db3b83b56bd66b619': Node(id='8e824ec8c0654d0db3b83b56bd66b619', data=RetrievalQA(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template="Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n{context}\n\nQuestion: {question}\nHelpful Answer:"), llm=GPT4All(model='models/my_little_llm.gguf', device='gpu', client=&lt;gpt4all.gpt4all.GPT4All object at 0x776da5594320&gt;)), document_variable_name='context'), retriever=VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=&lt;langchain_community.vectorstores.chroma.Chroma object at 0x776cf1b45be0&gt;, search_type='mmr'))),
 '4b55a6602f9542fe8d583ac66c6ae722': Node(id='4b55a6602f9542fe8d583ac66c6ae722', data=&lt;class 'pydantic.v1.main.ChainOutput'&gt;)}
</code></pre></div></div> <h3 id="retrieval-qa-with-custom-prompt-template">Retrieval Q&amp;A with custom prompt template</h3> <p>The example below shows how to edit the prompt template used by the chain, albeit, in this case, with limited success. This is due to the limited performance of the LLM being used.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>

<span class="c1"># Build prompt
</span><span class="n">template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">Use the following pieces of context to answer the question at the end. If you don</span><span class="sh">'</span><span class="s">t know the answer, just say that you don</span><span class="sh">'</span><span class="s">t know, don</span><span class="sh">'</span><span class="s">t try to make up an answer. At the end of the response, say </span><span class="se">\"</span><span class="s">over and out</span><span class="se">\"</span><span class="s">.
{context}
Question: {question}
Helpful Answer:</span><span class="sh">"""</span>
<span class="n">qa_chain_prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>
<span class="c1"># Run chain
</span><span class="n">qa_chain</span> <span class="o">=</span> <span class="n">RetrievalQA</span><span class="p">.</span><span class="nf">from_chain_type</span><span class="p">(</span>
    <span class="n">llm</span><span class="p">,</span>
    <span class="n">retriever</span><span class="o">=</span><span class="n">vectordb</span><span class="p">.</span><span class="nf">as_retriever</span><span class="p">(</span><span class="n">search_type</span><span class="o">=</span><span class="sh">"</span><span class="s">mmr</span><span class="sh">"</span><span class="p">),</span>
    <span class="n">return_source_documents</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">chain_type_kwargs</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">prompt</span><span class="sh">"</span><span class="p">:</span> <span class="n">qa_chain_prompt</span><span class="p">},</span>
<span class="p">)</span>
</code></pre></div></div> <p>Let’s ask a few questions to our Q&amp;A bot and render the output as some nice Markdown.</p> <p>Note that we can also output the documents that were retrieved during RAG and used to compose the answer, but that would pollute the output too much, so I left it commented out.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">IPython.display</span> <span class="kn">import</span> <span class="n">Markdown</span>

<span class="n">questions</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">"</span><span class="s">Given me the equation for the action value function update.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">What is the overall architecture of the Deep Q-Networks?</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">What is the difference between causal language modelling and masked language modelling?</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">What is zero-shot learning?</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Explain to me the concept of bucketing in RNNs.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">What is a named entity in the concept of NLP?</span><span class="sh">"</span><span class="p">,</span>
<span class="p">]</span>

<span class="k">for</span> <span class="n">q</span> <span class="ow">in</span> <span class="n">questions</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="nf">qa_chain</span><span class="p">({</span><span class="sh">"</span><span class="s">query</span><span class="sh">"</span><span class="p">:</span> <span class="n">q</span><span class="p">})</span>

    <span class="nf">display</span><span class="p">(</span><span class="nc">Markdown</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">**Question: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="sh">"</span><span class="s">query</span><span class="sh">"</span><span class="p">]</span><span class="si">}</span><span class="s">**</span><span class="se">\n\n</span><span class="s"> Answer: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="sh">'</span><span class="s">result</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">))</span>
    <span class="c1"># source_docs = "\n\n".join(d.page_content for d in result["source_documents"])
</span>    <span class="c1"># print(
</span>    <span class="c1">#     f"Source documents\n\n{source_docs}"
</span>    <span class="c1"># )
</span></code></pre></div></div> <p><strong>Question: Given me the equation for the action value function update.</strong></p> <p>Answer: The equation for the action-value function update is given by:</p> \[q_{\pi}(s,a) = R(s,a) + \&lt;dummy32001&gt;{ \gamma V_\pi (s') | s' \in S'}\] <p>where $R(s,a)$ is the reward received when taking action a in state s and $\gamma$ is the discount factor.</p> <p><strong>Question: What is the overall architecture of the Deep Q-Networks?</strong></p> <p>Answer: The overall architecture of a Deep Q-Network (DQN) consists of an input layer, multiple hidden layers with nonlinear activation functions, and an output layer. It uses experience replay to store past experiences for training purposes, and employs target networks to stabilize the learning process. over and out</p> <p><strong>Question: What is the difference between causal language modelling and masked language modelling?</strong></p> <p>Answer: Causal language modeling refers to a method where the model predicts the next token in a sequence based on the previous tokens. In contrast, masked language modeling involves randomly masking some tokens during training time and then trains the model to reconstruct the original text by predicting the masked tokens.</p> <p><strong>Question: What is zero-shot learning?</strong></p> <p>Answer: Zero-shot learning refers to a model’s ability to perform new tasks without being explicitly trained on those specific tasks or examples. In the context of large language models, it means that an AI can execute new tasks without needing any explicit training data for those tasks.</p> <p><strong>Question: Explain to me the concept of bucketing in RNNs.</strong></p> <p>Answer: Bucketing in RNNs refers to grouping or organizing input sequences into fixed-sized groups, called “buckets”, before processing them with an RNN model. This technique helps improve training efficiency and reduce padding by ensuring that each bucket contains a sufficient amount of randomness and variability while preventing it from being too large so as not to introduce excessive padding.</p> <p><strong>Question: What is a named entity in the concept of NLP?</strong></p> <p>Answer: In the context of Natural Language Processing (NLP), a named entity refers to a real-world object that can be denoted with a proper name. Examples are a person, location, organization, product. It can be abstract or have a physical existence.</p> <p>In some answers, the model has actually followed the instructions from the new prompt, but we need a much more powerful LLM, or the employment of techniques such as few-shot learning, to get better instruction-following results.</p> <h2 id="conclusions">Conclusions</h2> <p>This notebook presented a proof-of-concept on how to create a question-answering bot powered by an LLM and with knowledge extracted from actual documents, more specifically, a collection of notes from Obsidian.</p> <p>We were able to run all experiments locally, using a 7th gen i7, an NVIDIA GTX 1070 and 32 GBs of RAM.</p> <p>We were also introduced to many concepts on the road to building this Q&amp;A bot, such as:</p> <ol> <li>Document loading</li> <li>Text data cleaning using regex</li> <li>Splitting of Markdown documents into text chunks</li> <li>Sentence embeddings</li> <li>Vector stores</li> <li>Similarity search using cosine similarity between embedding vectors</li> <li>maximal marginal relevance search</li> <li>Self-query retrieval</li> <li>Contextual compression retrieval</li> <li>Question-answering with retrieval augmented generation</li> <li>Question-answering with retrieval augmented generation and custom templates</li> </ol> <p>In future work, let’s build an actual chatbot that remembers previous answers and can keep up a lengthier conversation.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/03/local-gemma-chatbot-langchain-ollama/">Running a Gemma-powered question-answering chatbot locally with LangChain + Ollama</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/07/ts-queue-experiments/">Using task-spooler to queue experiments on Linux</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/06/mathematics-self-study/">Resources to self-study mathematics for machine learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/08/tf-obj-tutorial/">How to train your own object detection models using the TensorFlow Object Detection API (2020 Update)</a> </li> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Douglas De Rizzo Meneghetti. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>